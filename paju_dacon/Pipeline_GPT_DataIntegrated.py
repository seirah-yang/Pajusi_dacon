# -*- coding: utf-8 -*-
"""
Pipeline_GPT_DataIntegrated.py
E2E ë¬¸ì„œ ìƒì„± íŒŒì´í”„ë¼ì¸ (Hybrid Retrieval + Validation + Export + RPA Stub)

"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import re, json, csv, time, sys

import numpy as np
from PyPDF2 import PdfReader
from pydantic import BaseModel, ValidationError
from rank_bm25 import BM25Okapi

# ===== (ì˜µì…˜) FAISS ì„ë² ë”© =====
try:
    import faiss
    from sentence_transformers import SentenceTransformer
    _FAISS_OK = True
except Exception:
    _FAISS_OK = False


# =========================
# 0) ë°ì´í„° ëª¨ë¸ & ìœ í‹¸
# =========================
class GenOutput(BaseModel):
    summary: str
    body: str
    recommendations: List[Dict[str, str]]
    action_items: List[Dict[str, str]]
    references: List[str]

def normalize_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def sentence_chunking(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """ë¬¸ì¥ ê¸°ë°˜ + overlap ì²­í‚¹ (í•œê¸€ ë¬¸ì„œ ìµœì í™”)"""
    if not text:
        return []
    # ë¬¸ì¥ ë¶„ë¦¬: ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ/ì¢…ê²° 'ë‹¤.' íŒ¨í„´ ê¸°ì¤€
    sents = re.split(r'(?<=[\.!\?]|ë‹¤\.)\s+', text)
    chunks, buf, cur_len = [], [], 0
    for sent in sents:
        sent = sent.strip()
        if not sent:
            continue
        buf.append(sent)
        cur_len += len(sent)
        if cur_len >= chunk_size:
            chunk = normalize_text(" ".join(buf))
            if len(chunk) >= 50:
                chunks.append(chunk)
            # overlap: ë’¤ì—ì„œ ì¼ì • ê¸¸ì´ ë¬¸ì¥ ìœ ì§€
            keep = []
            kept_len = 0
            for s in reversed(buf):
                keep.append(s)
                kept_len += len(s)
                if kept_len >= overlap:
                    break
            buf = list(reversed(keep))
            cur_len = sum(len(x) for x in buf)
    if buf:
        chunk = normalize_text(" ".join(buf))
        if len(chunk) >= 50:
            chunks.append(chunk)
    return chunks


# =========================
# 1) ì½”í¼ìŠ¤
# =========================
@dataclass
class DocRecord:
    doc_id: str
    title: str
    text: str
    meta: Dict[str, Any] = field(default_factory=dict)

class TextCorpus:
    def __init__(self):
        self.documents: List[Dict[str, Any]] = []
        self.chunks: List[str] = []
        self.records: List[DocRecord] = []
        self.chunk_meta: List[Dict[str, Any]] = []

    def add_pdf(self, file_path: str, title: Optional[str] = None, chunk_size: int = 500, overlap: int = 50):
        p = Path(file_path)
        reader = PdfReader(str(p))
        text = ""
        for page in reader.pages:
            text += page.extract_text() or ""
        text = normalize_text(text)
        doc_id = p.stem
        title = title or p.stem
        rec = DocRecord(doc_id=doc_id, title=title, text=text, meta={"type": "pdf", "path": str(p)})
        self.records.append(rec)
        self.documents.append({"path": str(p), "text": text})
        self._add_chunks_from_text(text, doc_id, title, "pdf", chunk_size, overlap)

    def add_txt(self, file_path: str, doc_type: str = "txt", title: Optional[str] = None,
                chunk_size: int = 500, overlap: int = 50, encoding: str = "utf-8"):
        p = Path(file_path)
        text = normalize_text(p.read_text(encoding=encoding))
        doc_id = p.stem
        title = title or p.stem
        rec = DocRecord(doc_id=doc_id, title=title, text=text, meta={"type": doc_type, "path": str(p)})
        self.records.append(rec)
        self.documents.append({"path": str(p), "text": text})
        self._add_chunks_from_text(text, doc_id, title, doc_type, chunk_size, overlap)

    def _add_chunks_from_text(self, text: str, doc_id: str, title: str, doc_type: str,
                              chunk_size: int, overlap: int):
        for i, ch in enumerate(sentence_chunking(text, chunk_size=chunk_size, overlap=overlap)):
            self.chunks.append(ch)
            self.chunk_meta.append({
                "doc_id": doc_id,
                "title": title,
                "type": doc_type,
                "chunk_id": i
            })

    def summary(self):
        print(f"ğŸ“š Documents: {len(self.documents)} | ğŸ§© Chunks: {len(self.chunks)}")


# =========================
# 2) Hybrid Retriever
# =========================
class HybridRetriever:
    def __init__(self, corpus: TextCorpus, embed_model_name: str = "intfloat/e5-large"):
        if not corpus.chunks:
            raise ValueError("âŒ Corpus has no chunks. Load PDFs/TXTs and chunk before initializing retriever.")
        self.corpus = corpus
        # BM25
        tokenized = [c.split() for c in corpus.chunks]
        self.bm25 = BM25Okapi(tokenized)
        # FAISS (ì˜µì…˜)
        self.has_faiss = _FAISS_OK
        if self.has_faiss:
            self.emb_model = SentenceTransformer(embed_model_name)
            self.emb_mat = self.emb_model.encode(
                corpus.chunks, convert_to_numpy=True, normalize_embeddings=True, batch_size=32, show_progress_bar=False
            )
            dim = self.emb_mat.shape[1]
            self.index = faiss.IndexFlatIP(dim)
            self.index.add(self.emb_mat)

    def search(self, query: str, topk: int = 6, alpha: float = 0.5) -> List[Tuple[int, float]]:
        # BM25
        token_q = query.split()
        bm25_scores = self.bm25.get_scores(token_q)
        b = (bm25_scores - bm25_scores.min()) / (bm25_scores.ptp() + 1e-8)
        # FAISS
        if self.has_faiss:
            q_emb = self.emb_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
            D, I = self.index.search(q_emb, topk)
            faiss_scores = np.zeros_like(b)
            for rank, idx in enumerate(I[0]):
                faiss_scores[idx] = D[0][rank]
            f = (faiss_scores - faiss_scores.min()) / (faiss_scores.ptp() + 1e-8)
        else:
            f = np.zeros_like(b)
        hybrid = alpha * b + (1 - alpha) * f
        top_idx = np.argsort(-hybrid)[:topk]
        return [(int(i), float(hybrid[i])) for i in top_idx]

    def build_context(self, query: str, k: int = 6) -> str:
        hits = self.search(query, topk=k, alpha=0.5)
        lines = []
        for idx, sc in hits:
            meta = self.corpus.chunk_meta[idx] if idx < len(self.corpus.chunk_meta) else {}
            title = meta.get("title", "NA")
            typ = meta.get("type", "NA")
            cid = meta.get("chunk_id", idx)
            lines.append(f"[{typ}/{title}#{cid}] {self.corpus.chunks[idx]}")
        return "\n\n".join(lines)


# =========================
# 3) Prompt Builder & LLM ìŠ¤í…
# =========================
def build_prompt(section_name: str, doc_type: str, constraints: List[str], references: str, query: str) -> str:
    return f"""
# ì‘ì„±í•­ëª©: [{section_name}]
# ë¬¸ì„œìœ í˜•: [{doc_type}]
# ì‘ì„±ì¡°ê±´: {', '.join(constraints)}
# ì°¸ê³ ìë£Œ(ìš”ì•½/ë°œì·Œ):
{references}

ë‹¤ìŒ ê¸°ì¤€ì— ë§ì¶° ë‚´ìš©ì„ ìƒì„±í•˜ì„¸ìš”:
1) ì¶œë ¥ í˜•ì‹: JSON. í‚¤:
{{
  "summary": "í•œ ë¬¸ë‹¨ ìš”ì•½",
  "body": "ìƒì„¸ ë³¸ë¬¸",
  "recommendations": [{{"title":"", "detail":"", "impact_estimate":""}}],
  "action_items": [{{"task":"", "owner":"", "due":"YYYY-MM-DD"}}],
  "references": ["ì¶œì²˜1", "ì¶œì²˜2"]
}}
2) ë³¸ë¬¸ ë‚´ ì£¼ì¥ ì˜†ì— (ê¸°ê´€, 20xx) í˜•íƒœì˜ ê°„ë‹¨ ì£¼ì„ì„ ìµœì†Œ 1íšŒ ì´ìƒ í‘œê¸°.
3) í•œêµ­ì–´ ê³µì‹ ë³´ê³ ì„œ ë¬¸ì²´ë¡œ ê°„ê²°í•˜ê³  ê·¼ê±° ê¸°ë°˜ìœ¼ë¡œ ì„œìˆ .
4) ì¿¼ë¦¬: "{query}"
""".strip()

def call_llm_stub(prompt: str) -> str:
    """ì™¸ë¶€ API ì—†ì´ ê·œê²©ì„ ë§Œì¡±í•˜ëŠ” ë”ë¯¸ JSONì„ ë°˜í™˜ (ìš´ì˜ ì‹œ ì‹¤ì œ LLMìœ¼ë¡œ êµì²´)"""
    dummy = {
        "summary": "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê·œì •Â·ì„œì‹ì„ í†µí•© ë¶„ì„í•˜ì—¬ í–‰ì •ë¬¸ì„œ ìë™í™”ë¥¼ ì„¤ê³„í•œë‹¤.",
        "body": "BM25ì™€ FAISSë¥¼ ê²°í•©í•´ ê·œì •/ì„œì‹ì„ ì •ë°€ íƒìƒ‰í•˜ê³ , RPAë¡œ ê²°ì¬/ì§‘ê³„/ì…ë ¥ì„ ìë™í™”í•œë‹¤. (íŒŒì£¼ì‹œ, 2025)",
        "recommendations": [
            {"title": "RPA ë‹¨ê³„ì  ë„ì…", "detail": "ë³´ê³ ì„œ ì§‘ê³„â†’ê²°ì¬ ì—°ë™â†’ì›ì¥ ë°˜ì˜ ìˆœì„œë¡œ ë‹¨ê³„ ì ìš©", "impact_estimate": "ì›” 30~45ì‹œê°„ ì ˆê°"}
        ],
        "action_items": [
            {"task": "ê²°ì¬ë©”ì¼ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ë°°í¬", "owner": "ì •ë³´í†µì‹ ê³¼", "due": "2025-11-15"}
        ],
        "references": ["íŒŒì£¼ì‹œ ì£¼ìš”ì—…ë¬´ê³„íš(2025)", "ì „ìì •ë¶€ í‘œì¤€í”„ë ˆì„ì›Œí¬ ê°€ì´ë“œ(2023)"]
    }
    return json.dumps(dummy, ensure_ascii=False, indent=2)


# =========================
# 4) Validation
# =========================
def validate_json_payload(js: str) -> GenOutput:
    try:
        obj = json.loads(js)
        return GenOutput(**obj)
    except (json.JSONDecodeError, ValidationError) as e:
        raise ValueError(f"[ValidationError] ìƒì„± JSONì´ ìŠ¤í‚¤ë§ˆì™€ ë‹¤ë¦…ë‹ˆë‹¤: {e}")

def check_reference_annotations(text: str) -> bool:
    """ë³¸ë¬¸ ë‚´ (ê¸°ê´€, 20xx) í˜•íƒœ ì£¼ì„ì´ ì¡´ì¬í•˜ëŠ”ì§€ ê°„ë‹¨ ì ê²€"""
    return bool(re.search(r"\([\wê°€-í£]+,\s*20\d{2}\)", text))


# =========================
# 5) Export
# =========================
def export_json(obj: GenOutput, out_path: str):
    Path(out_path).write_text(json.dumps(obj.dict(), ensure_ascii=False, indent=2), encoding="utf-8")

def export_csv_action_items(obj: GenOutput, csv_path: str):
    header = ["task", "owner", "due"]
    Path(csv_path).parent.mkdir(parents=True, exist_ok=True)
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=header)
        w.writeheader()
        for it in obj.action_items:
            w.writerow({k: it.get(k, "") for k in header})


# =========================
# 6) RPA Stubs
# =========================
def rpa_send_approval_mail(obj: GenOutput, to_addr: str):
    print(f"[RPA] ê²°ì¬ ë©”ì¼ ì „ì†¡ â†’ {to_addr}")
    print(f"ì œëª©: [ê²°ì¬ìš”ì²­] {obj.summary[:50]}...")
    print(f"ë³¸ë¬¸(ìš”ì•½): {obj.summary}")

def rpa_append_report_ledger(obj: GenOutput, ledger_csv: str):
    Path(ledger_csv).parent.mkdir(parents=True, exist_ok=True)
    file_exists = Path(ledger_csv).exists()
    with open(ledger_csv, "a", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        if not file_exists:
            w.writerow(["date", "summary", "n_action_items", "references"])
        now = time.strftime("%Y-%m-%d")
        w.writerow([now, normalize_text(obj.summary), len(obj.action_items), ";".join(obj.references)])

def rpa_fill_form_stub(obj: GenOutput):
    print("[RPA] í¼ ìë™ ì…ë ¥(ìŠ¤í…) â€” ì‹¤ì œ ìš´ì˜ í™˜ê²½ API/ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ì—°ë™ ì§€ì ")


# =========================
# 7) LangGraph-Style Nodes
# =========================
@dataclass
class PipelineState:
    query: str
    section_name: str
    doc_type: str
    constraints: List[str] = field(default_factory=list)
    references_ctx: str = ""
    draft_json: str = ""
    obj: Optional[GenOutput] = None

class DataIngestNode:
    def __init__(self, corpus_dir: str, chunk_size: int = 500, overlap: int = 50):
        self.corpus_dir = Path(corpus_dir)
        self.chunk_size = chunk_size
        self.overlap = overlap

    def __call__(self) -> TextCorpus:
        corpus = TextCorpus()
        if not self.corpus_dir.exists():
            raise FileNotFoundError(f"âŒ Corpus directory not found: {self.corpus_dir}")

        # PDF ìš°ì„  ë¡œë“œ
        for p in self.corpus_dir.glob("*.pdf"):
            corpus.add_pdf(str(p), chunk_size=self.chunk_size, overlap=self.overlap)
        # TXTë„ ì§€ì›
        for p in self.corpus_dir.glob("*.txt"):
            # íŒŒì¼ëª…ì— ë”°ë¼ ê·œì •/ì„œì‹/ê°€ì´ë“œë¼ì¸ íƒœê¹… ì˜ˆì‹œ
            dtype = "ê·œì •" if "reg" in p.stem else ("ì„œì‹" if "form" in p.stem else "ê°€ì´ë“œë¼ì¸")
            corpus.add_txt(str(p), doc_type=dtype, chunk_size=self.chunk_size, overlap=self.overlap)

        corpus.summary()
        if len(corpus.chunks) == 0:
            raise RuntimeError("âŒ No chunks loaded. Check PDF/TXT extraction and directory content.")
        return corpus

class ContextSearchNode:
    def __init__(self, retriever: HybridRetriever, k: int = 6):
        self.retriever = retriever
        self.k = k
    def __call__(self, st: PipelineState) -> PipelineState:
        st.references_ctx = self.retriever.build_context(st.query, k=self.k)
        return st

class DraftWriterNode:
    def __call__(self, st: PipelineState) -> PipelineState:
        prompt = build_prompt(st.section_name, st.doc_type, st.constraints, st.references_ctx, st.query)
        st.draft_json = call_llm_stub(prompt)  # ìš´ì˜ ì‹œ ì‹¤ì œ LLMìœ¼ë¡œ êµì²´
        return st

class ValidatorNode:
    def __call__(self, st: PipelineState) -> PipelineState:
        obj = validate_json_payload(st.draft_json)
        if not check_reference_annotations(obj.body):
            print("[WARN] ë³¸ë¬¸ì— (ê¸°ê´€, ì—°ë„) í˜•íƒœ ì£¼ì„ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ê°•í•˜ì„¸ìš”.")
        st.obj = obj
        return st

class ExporterNode:
    def __init__(self, out_dir: str = "./outputs"):
        self.out_dir = Path(out_dir)
        self.out_dir.mkdir(parents=True, exist_ok=True)
    def __call__(self, st: PipelineState) -> PipelineState:
        assert st.obj is not None, "Export ë‹¨ê³„ ì „ì— Validatorë¥¼ í†µê³¼í•´ì•¼ í•©ë‹ˆë‹¤."
        export_json(st.obj, str(self.out_dir / "generated.json"))
        export_csv_action_items(st.obj, str(self.out_dir / "action_items.csv"))
        print(f"[EXPORT] JSON/CSV ì €ì¥ ì™„ë£Œ â†’ {self.out_dir}")
        return st

class RPAOpsNode:
    def __init__(self, out_dir: str = "./outputs", approval_addr: str = "approval@org.local"):
        self.out_dir = Path(out_dir)
        self.approval_addr = approval_addr
    def __call__(self, st: PipelineState) -> PipelineState:
        assert st.obj is not None
        rpa_send_approval_mail(st.obj, to_addr=self.approval_addr)
        rpa_append_report_ledger(st.obj, str(self.out_dir / "report_ledger.csv"))
        rpa_fill_form_stub(st.obj)
        return st


# =========================
# 8) Orchestrator
# =========================
def run_pipeline(
    query: str,
    section_name: str = "ë„ì… ë°°ê²½ ë° í•„ìš”ì„±",
    doc_type: str = "í–‰ì • ìë™í™” ê¸°íšì„œ",
    constraints: Optional[List[str]] = None,
    corpus_dir: str = "/home/alpaco/homework/paju_dacon/corpus",
    out_dir: str = "./outputs",
    k_ctx: int = 6,
):
    constraints = constraints or ["ê·¼ê±° ê¸°ë°˜", "ì¤‘ë³µ ìµœì†Œí™”", "ì •ëŸ‰ì  ìˆ˜ì¹˜ í¬í•¨", "UNIEVAL ê¸°ì¤€ ì¤€ìˆ˜"]

    # 1) Data ingest
    ingest = DataIngestNode(corpus_dir=corpus_dir, chunk_size=500, overlap=50)
    corpus = ingest()

    # 2) Retriever
    retriever = HybridRetriever(corpus)

    # 3) Build state
    st = PipelineState(query=query, section_name=section_name, doc_type=doc_type, constraints=constraints)

    # 4) Nodes
    ctx_node = ContextSearchNode(retriever, k=k_ctx)
    draft_node = DraftWriterNode()
    val_node = ValidatorNode()
    exp_node = ExporterNode(out_dir=out_dir)
    rpa_node = RPAOpsNode(out_dir=out_dir, approval_addr="approval@org.local")

    # 5) Flow
    st = ctx_node(st)
    st = draft_node(st)
    st = val_node(st)
    st = exp_node(st)
    st = rpa_node(st)

    print("[DONE] ê²°ê³¼:", str(Path(out_dir) / "generated.json"))
    return st.obj


# =========================
# 9) Main
# =========================
if __name__ == "__main__":
    try:
        run_pipeline(
            query="í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ í†µí•´ í–‰ì •ë¬¸ì„œ ìë™í™”ë¥¼ ìœ„í•œ ê·œì •/ì„œì‹ ìš”ê±´ì„ í†µí•©",
            section_name="ë„ì… ë°°ê²½ ë° í•„ìš”ì„±",
            doc_type="í–‰ì • ìë™í™” ê¸°íšì„œ",
            constraints=["ê·¼ê±° ê¸°ë°˜", "ì¤‘ë³µ ìµœì†Œí™”", "ì •ëŸ‰ì  ìˆ˜ì¹˜ í¬í•¨", "UNIEVAL ê¸°ì¤€ ì¤€ìˆ˜"],
            corpus_dir="/home/alpaco/homework/paju_dacon/corpus",
            out_dir="./outputs",
            k_ctx=6,
        )
    except Exception as e:
        print("failed:", e, file=sys.stderr)
        sys.exit(1)